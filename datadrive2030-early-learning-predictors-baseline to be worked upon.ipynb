{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d892f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:00.876712Z",
     "iopub.status.busy": "2023-02-24T01:30:00.875296Z",
     "iopub.status.idle": "2023-02-24T01:30:02.734625Z",
     "shell.execute_reply": "2023-02-24T01:30:02.731939Z"
    },
    "papermill": {
     "duration": 1.870717,
     "end_time": "2023-02-24T01:30:02.737828",
     "exception": false,
     "start_time": "2023-02-24T01:30:00.867111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "Train = pd.read_csv('./Train.csv')\n",
    "Test = pd.read_csv('./Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1d0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:02.753050Z",
     "iopub.status.busy": "2023-02-24T01:30:02.752551Z",
     "iopub.status.idle": "2023-02-24T01:30:02.816647Z",
     "shell.execute_reply": "2023-02-24T01:30:02.815192Z"
    },
    "papermill": {
     "duration": 0.075486,
     "end_time": "2023-02-24T01:30:02.819910",
     "exception": false,
     "start_time": "2023-02-24T01:30:02.744424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf4596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:02.836368Z",
     "iopub.status.busy": "2023-02-24T01:30:02.835899Z",
     "iopub.status.idle": "2023-02-24T01:30:03.038861Z",
     "shell.execute_reply": "2023-02-24T01:30:03.037578Z"
    },
    "papermill": {
     "duration": 0.214719,
     "end_time": "2023-02-24T01:30:03.041950",
     "exception": false,
     "start_time": "2023-02-24T01:30:02.827231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Train.isnull().sum(), Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53eef45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:03.058603Z",
     "iopub.status.busy": "2023-02-24T01:30:03.058118Z",
     "iopub.status.idle": "2023-02-24T01:30:03.151772Z",
     "shell.execute_reply": "2023-02-24T01:30:03.150259Z"
    },
    "papermill": {
     "duration": 0.10514,
     "end_time": "2023-02-24T01:30:03.154687",
     "exception": false,
     "start_time": "2023-02-24T01:30:03.049547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Test.isnull().sum(), Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e48070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:03.171528Z",
     "iopub.status.busy": "2023-02-24T01:30:03.171061Z",
     "iopub.status.idle": "2023-02-24T01:30:03.199506Z",
     "shell.execute_reply": "2023-02-24T01:30:03.198130Z"
    },
    "papermill": {
     "duration": 0.040358,
     "end_time": "2023-02-24T01:30:03.202548",
     "exception": false,
     "start_time": "2023-02-24T01:30:03.162190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_derivation(float_df):\n",
    "        # 1. Calculate the child's age in years\n",
    "    float_df['child_age_years'] = float_df['child_age'] / 12\n",
    "\n",
    "    # 2. Calculate the percentage of children present\n",
    "    float_df['percentage_children_present'] = float_df['count_children_present'] / float_df['count_children_attendance'] * 100\n",
    "\n",
    "    # 3. Calculate the percentage of children pre-covid\n",
    "    float_df['percentage_children_precovid'] = float_df['count_children_precovid'] / float_df['count_children_attendance'] * 100\n",
    "\n",
    "    # 4. Calculate the difference between present and pre-covid children\n",
    "    float_df['diff_children_present_precovid'] = float_df['count_children_present'] - float_df['count_children_precovid']\n",
    "\n",
    "    # 5. Calculate the gender ratio (male / female)\n",
    "    float_df['gender_ratio'] = float_df['count_register_gender_male'] / float_df['count_register_gender_female']\n",
    "\n",
    "    # 6. Calculate the percentage of staff with different qualifications\n",
    "    float_df['percentage_staff_qual_skills'] = float_df['count_staff_qual_skills'] / float_df['count_staff_contract'] * 100\n",
    "    float_df['percentage_staff_qual_nqf4_5'] = float_df['count_staff_qual_nqf4_5'] / float_df['count_staff_contract'] * 100\n",
    "    float_df['percentage_staff_qual_nqf6_9'] = float_df['count_staff_qual_nqf6_9'] / float_df['count_staff_contract'] * 100\n",
    "\n",
    "    # 7. Calculate the ratio of teachers to children\n",
    "    float_df['teacher_child_ratio'] = float_df['count_staff_contract'] / float_df['count_children_present']\n",
    "\n",
    "    # 8. Calculate the ratio of full-time teachers to children\n",
    "    float_df['full_time_teacher_child_ratio'] = float_df['count_staff_time_full'] / float_df['count_children_present']\n",
    "\n",
    "    # 9. Calculate the average salary for paid staff\n",
    "    float_df['average_staff_salary'] = float_df['count_staff_salary'] / float_df['count_staff_salary_paid']\n",
    "\n",
    "    # 10. Calculate the percentage of full-time staff\n",
    "    float_df['percentage_full_time_staff'] = float_df['count_staff_time_full'] / float_df['count_staff_time'] * 100\n",
    "\n",
    "    # 11. Calculate the total number of toilets\n",
    "    float_df['total_toilets'] = float_df['count_toilets_children'] + float_df['count_toilets_adults']\n",
    "\n",
    "    # 12. Calculate the number of children per toilet\n",
    "    float_df['children_per_toilet'] = float_df['count_children_present'] / float_df['total_toilets']\n",
    "\n",
    "    # 13. Calculate the teacher's total score (sum of selfcare and emotional)\n",
    "    float_df['teacher_total_score'] = float_df['teacher_selfcare_total'] + float_df['teacher_emotional_total']\n",
    "\n",
    "    # 14. Calculate the percentage of male and female staff\n",
    "    float_df['percentage_female_staff'] = float_df['count_staff_gender_female'] / float_df['count_staff_gender'] * 100\n",
    "    \n",
    "    # 15. Calculate the difference between the number of registered children and the facility capacity\n",
    "    float_df['diff_registered_capacity'] = float_df['count_register_all'] - float_df['pri_capacity']\n",
    "\n",
    "    # 16. Calculate the percentage of registered children from different racial backgrounds\n",
    "    float_df['percentage_race_coloured'] = float_df['count_register_race_coloured'] / float_df['count_register_race'] * 100\n",
    "    float_df['percentage_race_indian'] = float_df['count_register_race_indian'] / float_df['count_register_race'] * 100\n",
    "    float_df['percentage_race_white'] = float_df['count_register_race_white'] / float_df['count_register_race'] * 100\n",
    "\n",
    "    # 17. Calculate the percentage of unpaid staff\n",
    "    float_df['percentage_unpaid_staff'] = (float_df['count_staff_all'] - float_df['count_staff_salary_paid']) / float_df['count_staff_all'] * 100\n",
    "\n",
    "    # 18. Calculate the difference in years between child registration (most recent year minus the registration year)\n",
    "    float_df['years_since_registration_2016'] = 2022 - 2016\n",
    "    float_df['years_since_registration_2015'] = 2022 - 2015\n",
    "\n",
    "    # 19. Calculate the percentage of children in each registration year\n",
    "    float_df['percentage_registered_2016'] = float_df['count_register_year_2016'] / float_df['count_register_year_school'] * 100\n",
    "    float_df['percentage_registered_2015'] = float_df['count_register_year_2015'] / float_df['count_register_year_school'] * 100\n",
    "\n",
    "    # 20. Calculate the difference between the facility's fees and the average fees in the same province\n",
    "    float_df['diff_fees_province'] = float_df['pri_fees_amount'] - float_df['pri_fees_amount_pv']\n",
    "\n",
    "    # 21. Calculate the percentage of children in different grader levels\n",
    "    float_df['percentage_registered_grader'] = float_df['count_register_year_grader'] / float_df['count_register_year_school'] * 100\n",
    "    \n",
    "    # 22. Calculate the average enrollment duration of children in months\n",
    "    float_df['average_months_enrollment'] = float_df['child_months_enrolment'] / float_df['count_children_present']\n",
    "\n",
    "    # 23. Calculate the ratio of registered children to the total number of children in the facility\n",
    "    float_df['registered_children_ratio'] = float_df['count_register_all'] / float_df['count_children_present']\n",
    "\n",
    "    # 24. Calculate the average observed total score per child\n",
    "    float_df['average_child_observe_total'] = float_df['child_observe_total'] / float_df['count_children_present']\n",
    "\n",
    "    # 25. Calculate the percentage of children with ZHA (Z-Score Height for Age)\n",
    "    float_df['percentage_children_zha'] = float_df['child_zha'] / float_df['count_children_present'] * 100\n",
    "\n",
    "    # 26. Calculate the ratio of observed classrooms to the total number of classrooms\n",
    "    float_df['observed_classroom_ratio'] = float_df['obs_classrooms'] / float_df['pri_capacity']\n",
    "\n",
    "    # 27. Calculate the ratio of ward best to the total number of wards\n",
    "    float_df['ward_best_ratio'] = float_df['ward_best'] / float_df['id_ward']\n",
    "\n",
    "    # 28. Calculate the average enumerator score\n",
    "    float_df['average_enumerator_score'] = float_df['id_enumerator'] / float_df['count_staff_all']\n",
    "\n",
    "    # 29. Calculate the closing time in minutes\n",
    "    float_df['pri_time_close_minutes'] = float_df['pri_time_close_hours'] * 60\n",
    "\n",
    "    # 30. Calculate the total time the facility is open in minutes\n",
    "    float_df['facility_open_time_minutes'] = float_df['pri_time_close_minutes'] - float_df['pri_time_open_minutes']\n",
    "    # 31. Calculate the average height per child age\n",
    "    float_df['average_height_per_age'] = float_df['child_height'] / float_df['child_age_years']\n",
    "\n",
    "    # 32. Calculate the ratio of best wards to total wards in the same province\n",
    "    float_df['best_ward_province_ratio'] = float_df['ward_best'] / float_df['id_prov_n']\n",
    "\n",
    "    # 33. Calculate the ratio of best wards to total wards in the same municipality\n",
    "    float_df['best_ward_municipality_ratio'] = float_df['ward_best'] / float_df['id_mn_n']\n",
    "\n",
    "    # 34. Calculate the ratio of best wards to total wards in the same district\n",
    "    float_df['best_ward_district_ratio'] = float_df['ward_best'] / float_df['id_dc_n']\n",
    "\n",
    "    # 35. Calculate the ratio of children per enumerator\n",
    "    float_df['children_per_enumerator'] = float_df['count_children_present'] / float_df['id_enumerator']\n",
    "\n",
    "    # 36. Calculate the ratio of children per facility in the same municipality\n",
    "    float_df['children_per_facility_municipality'] = float_df['count_children_present'] / float_df['id_mn_n']\n",
    "\n",
    "    # 37. Calculate the ratio of children per facility in the same district\n",
    "    float_df['children_per_facility_district'] = float_df['count_children_present'] / float_df['id_dc_n']\n",
    "\n",
    "    # 38. Calculate the ratio of children per facility in the same province\n",
    "    float_df['children_per_facility_province'] = float_df['count_children_present'] / float_df['id_prov_n']\n",
    "\n",
    "    # 39. Calculate the ratio of staff per enumerator\n",
    "    float_df['staff_per_enumerator'] = float_df['count_staff_all'] / float_df['id_enumerator']\n",
    "    float_df['percentage_male_staff'] = float_df['count_staff_gender_male'] / float_df['count_staff_gender'] * 100\n",
    "    \n",
    "    \n",
    "    return float_df\n",
    "\n",
    "\n",
    "Train=feature_derivation(Train)\n",
    "Test=feature_derivation(Test)\n",
    "\n",
    "Test.shape,Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa0501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:03.220383Z",
     "iopub.status.busy": "2023-02-24T01:30:03.219859Z",
     "iopub.status.idle": "2023-02-24T01:30:03.235208Z",
     "shell.execute_reply": "2023-02-24T01:30:03.233881Z"
    },
    "papermill": {
     "duration": 0.027859,
     "end_time": "2023-02-24T01:30:03.238207",
     "exception": false,
     "start_time": "2023-02-24T01:30:03.210348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VariableDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bec54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:03.256262Z",
     "iopub.status.busy": "2023-02-24T01:30:03.255784Z",
     "iopub.status.idle": "2023-02-24T01:30:03.836748Z",
     "shell.execute_reply": "2023-02-24T01:30:03.834610Z"
    },
    "papermill": {
     "duration": 0.594153,
     "end_time": "2023-02-24T01:30:03.840533",
     "exception": false,
     "start_time": "2023-02-24T01:30:03.246380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = []; cat_features = []; not_features = []\n",
    "for k in Train.columns[1:]:\n",
    "    if Train[k].isnull().sum() < 6000:\n",
    "        features.append(k)\n",
    "        if Train[k].dtype == 'O':\n",
    "            cat_features.append(k)\n",
    "            print('There is '+ str(len(Train[k].value_counts()))+' Class in: ' +k)\n",
    "    else:\n",
    "        not_features.append(k)\n",
    "\n",
    "print('----------------------------------')\n",
    "print('We have '+str(len(features)) + ' features')\n",
    "print('We have '+str(len(cat_features)) + ' categorical features')\n",
    "print('We have '+str(len(not_features)) + ' features that have more than 6000 of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96abe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:03.859621Z",
     "iopub.status.busy": "2023-02-24T01:30:03.859176Z",
     "iopub.status.idle": "2023-02-24T01:30:03.913641Z",
     "shell.execute_reply": "2023-02-24T01:30:03.912325Z"
    },
    "papermill": {
     "duration": 0.067596,
     "end_time": "2023-02-24T01:30:03.916805",
     "exception": false,
     "start_time": "2023-02-24T01:30:03.849209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'target' in features\n",
    "Train = Train[features]\n",
    "features.remove('target')\n",
    "Test  = Test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632d745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:03.935316Z",
     "iopub.status.busy": "2023-02-24T01:30:03.934860Z",
     "iopub.status.idle": "2023-02-24T01:30:03.945109Z",
     "shell.execute_reply": "2023-02-24T01:30:03.943904Z"
    },
    "papermill": {
     "duration": 0.022446,
     "end_time": "2023-02-24T01:30:03.947658",
     "exception": false,
     "start_time": "2023-02-24T01:30:03.925212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(Train['target']), min(Train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6799fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape,Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_cols(df):\n",
    "    \"\"\"\n",
    "    Returns the number of columns with missing values in the given pandas DataFrame.\n",
    "    \"\"\"\n",
    "    missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "    return len(missing_cols)\n",
    "\n",
    "def column_respective_missing_values(df):\n",
    "  # Loop through each feature in the dataframe\n",
    "  for feature in df.columns:\n",
    "      # Count the number of missing values for the feature\n",
    "      num_missing = df[feature].isnull().sum()\n",
    "      \n",
    "      # Print the number of missing values for the feature\n",
    "      print(\"'{}':  {} missing values\".format(feature, num_missing))\n",
    "    \n",
    "def count_cols_by_dtype(df):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of the count of columns grouped by their data type in the given pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # get columns grouped by their data type\n",
    "    cols_by_dtype = df.columns.to_series().groupby(df.dtypes).groups\n",
    "    \n",
    "    # count number of columns for each data type\n",
    "    count_by_dtype = {}\n",
    "    for dtype, cols in cols_by_dtype.items():\n",
    "        count_by_dtype[dtype.name] = len(cols)\n",
    "    \n",
    "    return count_by_dtype\n",
    "\n",
    "\n",
    "def to_date_type(df):\n",
    "  # loop over all columns and try to convert them to datetime\n",
    "  for col in df.columns:\n",
    "      if df[col].dtype == 'object':\n",
    "          try:\n",
    "              df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
    "          except ValueError:\n",
    "              pass\n",
    "            \n",
    "\n",
    "def map_binary_cat(df):\n",
    "  for col in df.columns:\n",
    "      # check if the column is an object and has two unique values\n",
    "      if df[col].dtype == 'O' and len(df[col].unique()) == 2:\n",
    "          if set(df[col].unique()) == {'Yes', 'No'}:\n",
    "              # map 'Yes' to 1 and 'No' to 0\n",
    "              df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "          elif set(df[col].unique()) == {'True', 'False'}:\n",
    "              # map 'True' to 1 and 'False' to 0\n",
    "              df[col] = df[col].map({'True': 1, 'False': 0})\n",
    "          elif set(df[col].unique()) == {'Male', 'Female'}:\n",
    "              # map 'Male' to 1 and 'Female' to 0\n",
    "              df[col] = df[col].map({'Male': 1, 'Female': 0})\n",
    "          # add additional mapping statements here for other binary variables as needed\n",
    "\n",
    "\n",
    "def map_binary_to_binary(dataframe):\n",
    "    \"\"\"\n",
    "    Maps binary values in object columns of a pandas DataFrame to 1 and 0.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The DataFrame to be mapped.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The mapped DataFrame.\n",
    "    \"\"\"\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object':\n",
    "            unique_values = dataframe[column].unique()\n",
    "            if len(unique_values) == 2:\n",
    "                binary_values = [val for val in unique_values if val in ['Yes', 'No']]\n",
    "                if len(binary_values) == 2:\n",
    "                    dataframe[column] = dataframe[column].map({'Yes': 1, 'No': 0})\n",
    "    return dataframe\n",
    "\n",
    "def count_columns_with_missing_values(df,tresh):\n",
    "    \"\"\"\n",
    "    Given a pandas DataFrame, returns the number of columns that have more than 2710 missing values.\n",
    "    \"\"\"\n",
    "    # calculate the number of missing values in each column\n",
    "    missing_values = df.isna().sum()\n",
    "\n",
    "    # count the number of columns with more than 2710 missing values\n",
    "    count = (missing_values > tresh).sum()\n",
    "\n",
    "    return count\n",
    "\n",
    "def group_dataset_with_datatype(traindataset):\n",
    "    grouped = traindataset.columns.to_series().groupby(traindataset.dtypes)\n",
    "\n",
    "    # Create a new DataFrame for each data type group\n",
    "    float_df = traindataset[grouped.groups[np.dtype('float64')]]\n",
    "    date_df = traindataset[grouped.groups[np.dtype('datetime64[ns]')]]\n",
    "    int_df = traindataset[grouped.groups[np.dtype('int64')]]\n",
    "    object_df = traindataset[grouped.groups[np.dtype('object')]]\n",
    "    \n",
    "    return float_df, date_df, int_df, object_df\n",
    "\n",
    "def imputing_missing_values(float_df):\n",
    "    float_cols = float_df.columns[float_df.isnull().any()].tolist()\n",
    "    # Fill missing values for each column with the column median\n",
    "    for col in float_cols:\n",
    "        float_df_copy = float_df.copy()  # Make a copy of the DataFrame\n",
    "        float_df_copy.loc[float_df_copy[col].isnull(), col] = float_df[col].mean()  # Modify the copy using .loc[]\n",
    "        float_df = float_df_copy.copy()  # Assign the modified copy back to the original DataFrame\n",
    "    return float_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "\n",
    "to_date_type(Train)\n",
    "to_date_type(Test)\n",
    "\n",
    "############################################\n",
    "\n",
    "print(f\"Number of columns with missig values in train dataset: {count_missing_cols(Train)} columns\")\n",
    "print(f\"Number of columns with missig values in test dataset: {count_missing_cols(Test)} columns\")\n",
    "print(f'{count_cols_by_dtype(Test)}')\n",
    "print(f'{count_cols_by_dtype(Train)}')\n",
    "\n",
    "############################################\n",
    "\n",
    "float_df, date_df, int_df, object_df=group_dataset_with_datatype(Train)\n",
    "test_float_df, test_date_df, test_int_df, test_object_df=group_dataset_with_datatype(Test)\n",
    "print(float_df.shape, date_df.shape, int_df.shape, object_df.shape)\n",
    "print(test_float_df.shape, test_date_df.shape, test_int_df.shape, test_object_df.shape)\n",
    "\n",
    "\n",
    "############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf4934",
   "metadata": {},
   "source": [
    " ### 1. Imputing missing values for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_df=imputing_missing_values(float_df)\n",
    "test_float_df=imputing_missing_values(test_float_df)\n",
    "int_df=imputing_missing_values(int_df)\n",
    "test_int_df=imputing_missing_values(test_int_df)\n",
    "drop='percentage_children_zha'\n",
    "float_df.drop(drop,axis=1,inplace=True)\n",
    "test_float_df.drop(drop,axis=1,inplace=True)\n",
    "\n",
    "print(f\"float_df missing: {count_missing_cols(float_df)} columns\")\n",
    "print(f\"tet_float_df missing: {count_missing_cols(test_float_df)} columns\")\n",
    "print(f\"int_df missing: {count_missing_cols(int_df)} columns\")\n",
    "print(f\"int_df missing: {count_missing_cols(test_int_df)} columns\")                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae0a27",
   "metadata": {},
   "source": [
    "### 2. Imputing missing values in date variables and creating new date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05916175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_dates(df):\n",
    "    for col in df.columns:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        \n",
    "data_to_drop=date_df.columns.tolist()\n",
    "data_to_drop\n",
    "\n",
    "for col in date_df.columns:\n",
    "        date_df[f'{col}_year'] = date_df[col].dt.year\n",
    "        date_df[f'{col}_month'] = date_df[col].dt.month\n",
    "        \n",
    "for col in test_date_df.columns:\n",
    "        test_date_df[f'{col}_year'] = test_date_df[col].dt.year\n",
    "        test_date_df[f'{col}_month'] = test_date_df[col].dt.month\n",
    "\n",
    "\n",
    "date_df.drop(data_to_drop,axis=1,inplace=True)\n",
    "test_date_df.drop(data_to_drop,axis=1,inplace=True)\n",
    "\n",
    "impute_missing_dates(date_df)\n",
    "impute_missing_dates(test_date_df)\n",
    "print(f\"Number of columns with missig values in train dataset: {count_missing_cols(date_df)} columns\")\n",
    "print(f\"Number of columns with missig values in test dataset: {count_missing_cols(test_date_df)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_df.shape,date_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c67e19",
   "metadata": {},
   "source": [
    "### 3. Imputing object variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e781558",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df = object_df.astype('category')\n",
    "test_object_df = object_df.astype('category')\n",
    "\n",
    "\n",
    "test_cat_cols = test_object_df.select_dtypes(include=['category']).columns\n",
    "\n",
    "# iterate over each categorical column and impute missing values using mode imputation\n",
    "for col in test_cat_cols:\n",
    "            test_object_df[col] = test_object_df[col].cat.add_categories(\"nonvalue\").fillna(\"nonvalue\")\n",
    "            object_df[col] =object_df[col].cat.add_categories(\"nonvalue\").fillna(\"nonvalue\")\n",
    "            \n",
    "print(f\"Number of columns with missig values in test dataset: {count_missing_cols(test_object_df)} columns\")\n",
    "print(f\"Number of columns with missig values in test dataset: {count_missing_cols(object_df)} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0432c",
   "metadata": {},
   "source": [
    "### 4. Concatinating both numerical, categorical columns for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.concat([int_df, float_df, date_df, object_df], axis=1)\n",
    "Test = pd.concat([test_int_df, test_float_df, test_date_df, test_object_df], axis=1)\n",
    "Train.shape,Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = Train.corr()\n",
    "\n",
    "# Identify the features with low correlation values\n",
    "low_corr_features = []\n",
    "for col in corr_matrix.columns:\n",
    "    if corr_matrix.loc[col, \"target\"] < 0.1:  # Adjust threshold as needed\n",
    "        low_corr_features.append(col)\n",
    "\n",
    "# Remove the identified features from the DataFrame\n",
    "Train = Train.drop(low_corr_features, axis=1)\n",
    "Test=Test.drop(low_corr_features,axis=1)\n",
    "\n",
    "Train.shape,Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{set(Train.columns)-set(Test.columns)}')\n",
    "print(f'{set(Test.columns)-set(Train.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=Train.select_dtypes(include=('category')).columns\n",
    "len(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82121842",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Train.target\n",
    "Train=Train.drop('target',axis=1)\n",
    "X=Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits =40\n",
    "# Initialize the k-fold cross-validator\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "\n",
    "# Initialize an empty list to store the cross-validation results\n",
    "rmses = []\n",
    "fold_pred = []\n",
    "\n",
    "\n",
    "params1 = {'learning_rate': 0.001, 'subsample': 0.02,\n",
    "           'colsample_bytree': 0.2, 'max_depth': 30,\n",
    "           'objective':'rmse' }\n",
    "# params1 = {'learning_rate': 0.02, 'subsample': 0.05,\n",
    "#            'colsample_bytree': 0.2, 'max_depth': 30,\n",
    "#            'objective':'rmse' }\n",
    "\n",
    "\n",
    "# params1 = {'learning_rate': 0.015, 'subsample': 0.03,\n",
    "#            'colsample_bytree': 0.2, 'max_depth': 30,\n",
    "#            'objective':'rmse' }\n",
    "\n",
    "# params1 = {'learning_rate': 0.010592472218053005, 'subsample': 0.0172713541530352,\n",
    "#            'colsample_bytree': 0.2, 'max_depth': 30,\n",
    "#            'objective':'rmse' }\n",
    "\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(f'Fold {fold+1}')\n",
    "\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n",
    "\n",
    "    # Convert the training and validation data into LightGBM format\n",
    "    train_dataset = lgb.Dataset(data=X_train, label=y_train,categorical_feature=cat_features)\n",
    "    val_dataset = lgb.Dataset(data=X_val, label=y_val,categorical_feature=cat_features)\n",
    "    \n",
    "#     ,categorical_feature=categorical_columns\n",
    "    # Train the model on the current fold\n",
    "    model = LGBMRegressor(**params1, n_estimators= 40000, random_state=123, verbose=300)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=300)\n",
    "\n",
    "    # Generate predictions on the validation set for the current fold\n",
    "    preds_val = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "\n",
    "    # Calculate the RMSE for the current fold and store it in the list\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    print(f'Fold {fold+1} RMSE: {rmse:.4f}\\n')\n",
    "    rmses.append(rmse)\n",
    "\n",
    "    # Generate predictions on the test set for the current fold\n",
    "    p2 = model.predict(Test[X.columns], num_iteration=model.best_iteration_)\n",
    "    fold_pred.append(p2)\n",
    "\n",
    "# Calculate the mean RMSE across all folds\n",
    "mean_rmse = np.mean(rmses)\n",
    "print(f'Mean RMSE: {mean_rmse:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bb294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9af71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acddb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b612b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679af33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:04.329753Z",
     "iopub.status.busy": "2023-02-24T01:30:04.328428Z",
     "iopub.status.idle": "2023-02-24T01:30:04.548530Z",
     "shell.execute_reply": "2023-02-24T01:30:04.547153Z"
    },
    "papermill": {
     "duration": 0.233627,
     "end_time": "2023-02-24T01:30:04.551806",
     "exception": false,
     "start_time": "2023-02-24T01:30:04.318179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    Train[Train.columns[:-1]],\n",
    "    Train[Train.columns[-1]],\n",
    "    test_size = 0.15,\n",
    "    random_state = 42,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78626b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:04.573416Z",
     "iopub.status.busy": "2023-02-24T01:30:04.572952Z",
     "iopub.status.idle": "2023-02-24T01:30:04.581432Z",
     "shell.execute_reply": "2023-02-24T01:30:04.580137Z"
    },
    "papermill": {
     "duration": 0.023058,
     "end_time": "2023-02-24T01:30:04.584100",
     "exception": false,
     "start_time": "2023-02-24T01:30:04.561042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xtrain.shape, xvalid.shape, ytrain.shape, yvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd816a02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T01:30:04.603572Z",
     "iopub.status.busy": "2023-02-24T01:30:04.603118Z",
     "iopub.status.idle": "2023-02-24T01:47:51.393944Z",
     "shell.execute_reply": "2023-02-24T01:47:51.392536Z"
    },
    "papermill": {
     "duration": 1066.8035,
     "end_time": "2023-02-24T01:47:51.396465",
     "exception": false,
     "start_time": "2023-02-24T01:30:04.592965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor, Pool\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# # xtest = Test[features[:-1]]\n",
    "# train_dataset = Pool(data = xtrain, label = ytrain, cat_features=cat_features)\n",
    "# val_dataset   = Pool(data = xvalid, label = yvalid, cat_features=cat_features)\n",
    "# model         = CatBoostRegressor(iterations = 30000, learning_rate=0.1, random_seed=123, verbose=300)\n",
    "# model.fit(train_dataset, eval_set=val_dataset, use_best_model=True, early_stopping_rounds=300)\n",
    "# preds_valid = model.predict(xvalid)\n",
    "# preds_test  = model.predict(xtest)\n",
    "# print(np.sqrt(mean_squared_error(yvalid, preds_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestRegressor and mean_squared_error from scikit-learn\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Initialize a random forest regressor with some hyperparameters\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "# # Train the model on the training set\n",
    "# rf_model.fit(xtrain, ytrain)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred = rf_model.predict(xvalid)\n",
    "# # Calculate the RMSE between the predicted and true values\n",
    "# rmse = mean_squared_error(yvalid, y_pred, squared=False)\n",
    "\n",
    "# # Print the RMSE\n",
    "# print(f\"RMSE on the validation set: {rmse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1097.070197,
   "end_time": "2023-02-24T01:48:07.357641",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-24T01:29:50.287444",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
