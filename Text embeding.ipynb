{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb352812-f6a0-4c3f-84b7-7bf912b45b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ./requirement.txt -q\n",
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b194c-fc05-482a-81ea-1458fbd74e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e535587d-4d29-4859-ab31-5f9003829080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from chromadb.config import Settings\n",
    "from urllib.error import HTTPError\n",
    "from dataclasses import replace\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken # OpenAI's open-source tokenizer\n",
    "import chromadb\n",
    "import logging\n",
    "import random # to sample multiple elements from a list\n",
    "import arxiv\n",
    "import time\n",
    "import os # operating system dependent functionality, to walk through directories and files\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # recursively tries to split by different characters to find one that works\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader # loads pdfs from a given directory\n",
    "from langchain.chains import ConversationalRetrievalChain # looks up relevant documents from the retriever per history and question.\n",
    "from langchain.text_splitter import CharacterTextSplitter # splits the content\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings # wrapper for HuggingFaceBgeEmbeddings models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import ArxivLoader # loads paper for a given id from Arxiv\n",
    "from langchain.document_loaders import PyPDFLoader # loads a given pdf\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader # loads a given text\n",
    "from langchain.retrievers import ArxivRetriever # loads relevant papers for a given paper id from Arxiv\n",
    "from chromadb.utils import embedding_functions # loads Chroma's embedding functions from OpenAI, HuggingFace, SentenceTransformer and others\n",
    "from langchain.chat_models import ChatOpenAI # wrapper around OpenAI LLMs\n",
    "from langchain.vectorstores import Chroma # wrapper around ChromaDB embeddings platform\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFaceHub # wrapper around HuggingFaceHub models\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "load_dotenv() # loads env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d0ed74-4f19-45e3-b33f-7bb476593cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir arxiv_papers\n",
    "# dirpath = \"arxiv_papers\"\n",
    "\n",
    "# search = arxiv.Search(\n",
    "#   query = \"2303.18223\" # ID of the paper A Survey of Large Language Models\n",
    "# )\n",
    "\n",
    "# for result in tqdm(search.results()):\n",
    "#     result.download_pdf(dirpath=dirpath)\n",
    "#     print(f\"-> Paper id {result.get_short_id()} with title '{result.title}' is downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ddca51-c365-40db-aeac-bb29451e83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages loaded:  124\n"
     ]
    }
   ],
   "source": [
    "# papers = []\n",
    "# loader = DirectoryLoader('./arxiv_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "# papers = loader.load()\n",
    "# print(\"Total number of pages loaded: \", len(papers)) # Total number of pages loaded:  85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e30745-6d0d-4e71-8bd0-82f036605ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "with open('churchill.txt') as f:\n",
    "    churchill_speech = f.read()\n",
    "\n",
    "paper_chunks = text_splitter.split_documents(churchill_speech)\n",
    "paper_chunks =text_splitter.create_documents([churchill_speech])\n",
    "len(paper_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411144f-7730-4878-9f41-a397ff10fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veifying the average length of the chunks\n",
    "chunk_lengths = [len(paper_chunk.page_content) for paper_chunk in paper_chunks]\n",
    "np.average(chunk_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d07d4-65ea-46e0-a294-da953a6d10b4",
   "metadata": {},
   "source": [
    "# Embedding wih BG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5b16a8-02bf-4ad5-bb64-c19c9c350a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading HuggingFace BG embeddings\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    # model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151c3a2-f475-409b-9e85-337b56b407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"import torch; print(torch.cuda.is_available())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86798f82-240d-468e-bfd3-b5130bed0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory=\"./chromadb/\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=paper_chunks, # text data that you want to embed and store\n",
    "    embedding=embedding_function, # used to convert the documents into embeddings\n",
    "    persist_directory=persist_directory, # tells Chroma where to store its data\n",
    "    collection_name=\"arxiv_papers\" #  gives a name to the collection of embeddings, which will be helpful for retrieving specific groups of embeddings later.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8904010c-2031-471c-9d9a-4abe4d6cccef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectordb\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82479d2-47f6-46c1-8366-804f8878502d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
